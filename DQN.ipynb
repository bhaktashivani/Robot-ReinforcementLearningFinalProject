{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import tensorflow\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random \n",
    "import sys\n",
    "import time \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from collections import deque \n",
    "from collections import namedtuple, defaultdict\n",
    "import gym \n",
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space size: 4, State Space size: 8\n"
     ]
    }
   ],
   "source": [
    "print('Action Space size: {}, State Space size: {}'.format(env.action_space.n, env.observation_space.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Network for (Deep Q-Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, seed): \n",
    "        \n",
    "        super(DQNet, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.num_state = env.observation_space.shape[0]\n",
    "        self.num_action = env.action_space.n\n",
    "        \n",
    "        self.layer_1 = nn.Linear(self.num_state, 32)\n",
    "        self.layer_2 = nn.Linear(32, 64)\n",
    "        self.layer_3 = nn.Linear(64, self.num_action)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "        \n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x =  self.layer_3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    '''\n",
    "    This class is for the experience replay where we store the agent's experiences at each time step in a data set. \n",
    "    polled over many episodes into a replay memory. \n",
    "    '''\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \n",
    "        '''\n",
    "        param: buffer_size : Size for memory initialization\n",
    "        param: batch_size : batch size \n",
    "        param: seed : random seed value to be set. \n",
    "\n",
    "        '''\n",
    "        \n",
    "        assert isinstance(buffer_size, int)\n",
    "        assert isinstance(batch_size, int)\n",
    "        assert isinstance(seed, int)\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = deque(maxlen = buffer_size)\n",
    "        self.batch_size = batch_size \n",
    "        self.experience = namedtuple(\"Experience\", field_names = ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        \n",
    "    \n",
    "    def buffer_add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        '''  Add experience to the memory and experience tuple.  '''\n",
    "        \n",
    "        experience = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(experience)\n",
    "        \n",
    "    def buffer_sample(self): \n",
    "        '''\n",
    "        randomly sample experience from the memory and return it\n",
    "        '''\n",
    "        \n",
    "        exp = random.sample(self.memory, k = self.batch_size)\n",
    "        \n",
    "        # convert the tuple into seperate stacks for each observation. \n",
    "        state  = torch.from_numpy(np.vstack( [e.state  for e in exp] )).float().to(device)\n",
    "        action = torch.from_numpy(np.vstack( [e.action for e in exp] )).long().to(device)\n",
    "        reward = torch.from_numpy(np.vstack( [e.reward for e in exp] )).float().to(device)\n",
    "        next_s = torch.from_numpy(np.vstack( [e.next_state for e in exp] )).float().to(device)\n",
    "        done   = torch.from_numpy(np.vstack( [int(e.done)  for e in exp] )).float().to(device)\n",
    "        \n",
    "        return (state, action, reward, next_s, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent: \n",
    "    \n",
    "    def __init__(self, env, lr,  seed):\n",
    "        \n",
    "        self.num_state = env.observation_space.shape[0]\n",
    "        self.num_action = env.action_space.n\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.q_net = DQNet(env,seed).to(device)\n",
    "        self.q_net_target = DQNet(env,seed).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr = lr)\n",
    "        \n",
    "        self.time_step = 0\n",
    "        \n",
    "    def update_buffer(self,  state, action, reward, new_state, done):\n",
    "        '''\n",
    "        add the experience to the replay buffer memory. \n",
    "        '''\n",
    "        # add the current experience to the replay buffer memory \n",
    "        self.buffer.buffer_add(state, action, reward, new_state, done)\n",
    "        self.time_step += 1 \n",
    "        \n",
    "        if self.time_step % UPDATE_EVERY == 0: \n",
    "            if len(self.buffer.memory) > BATCH_SIZE:\n",
    "                sample_experience = self.buffer.buffer_sample()\n",
    "                self.train(sample_experience)\n",
    "    \n",
    "    def get_action(self,  state, epsilon = 0.0):\n",
    "        \n",
    "        ''' get action for the given state, will use e-greedy policy method to select our action'''\n",
    "        \n",
    "        # Choose probability epsilon, select random action \n",
    "        if random.random() < epsilon: \n",
    "            action = np.random.randint(self.num_action)\n",
    "            return action\n",
    "        \n",
    "        # else select the maz of Q function. \n",
    "        else: \n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            self.q_net.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.q_net(state)\n",
    "            self.q_net.train()    \n",
    "            action = np.argmax(action.cpu().data.numpy())\n",
    "            return action \n",
    "    \n",
    "    def update_target_networks(self, target_model, source_model, tau=1e-3):\n",
    "        \n",
    "        ''' Function to update the target network using the current network parameters and Tau '''\n",
    "        \n",
    "        for target_param, source_param in zip(target_model.parameters(), source_model.parameters()):\n",
    "            target_param.data.copy_(target_param.data*(1.0 - tau) + source_param.data*tau)  \n",
    "        \n",
    "    \n",
    "    def train(self, experience):\n",
    "        \n",
    "        '''\n",
    "        Train the DQ Network  \n",
    "        '''\n",
    "           \n",
    "        state, action, reward, next_state, done = experience # experience from Replay memory \n",
    "       \n",
    "        # get the action for current states\n",
    "        self.q_net.train()\n",
    "        current_Q = self.q_net(state).gather(dim = 1, index = action)\n",
    "        \n",
    "        # get the next action for next states \n",
    "        self.q_net.eval()\n",
    "        with torch.no_grad():\n",
    "            tar_Q = self.q_net_target(next_state) #.detach()\n",
    "        max_tar_Q = tar_Q.data.max(1)[0].unsqueeze(1)\n",
    "        # execute the action in emulator and observe reward and image \n",
    "        \n",
    "        Q_target = reward + (GAMMA * max_tar_Q * (1 - done))\n",
    "        \n",
    "        # Calculare loss and update parameters for q_net \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(current_Q, Q_target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network parameters \n",
    "        self.update_target_networks(self.q_net_target, self.q_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64 \n",
    "GAMMA = 0.99\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "epsilon = 1.0      \n",
    "delta_epsilon = 0.999    \n",
    "min_epsilon = 0.01      \n",
    "\n",
    "num_episodes = 2500\n",
    "num_steps = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 100/3500, Average score = -173.2327\n",
      "Epochs 200/3500, Average score = -157.8047\n",
      "Epochs 300/3500, Average score = -149.6496\n",
      "Epochs 400/3500, Average score = -104.8623\n",
      "Epochs 500/3500, Average score = -83.1956\n",
      "Epochs 600/3500, Average score = -62.7826\n",
      "Epochs 700/3500, Average score = -54.1742\n",
      "Epochs 800/3500, Average score = -26.2052\n",
      "Epochs 900/3500, Average score = -17.5114\n",
      "Epochs 1000/3500, Average score = 6.2928\n",
      "Epochs 1100/3500, Average score = 53.5844\n",
      "Epochs 1200/3500, Average score = 81.3546\n",
      "Epochs 1300/3500, Average score = 91.2661\n",
      "Epochs 1400/3500, Average score = 111.9538\n",
      "Epochs 1500/3500, Average score = 116.0861\n",
      "Epochs 1600/3500, Average score = 129.1786\n",
      "Epochs 1700/3500, Average score = 191.7495\n",
      "Epochs 1800/3500, Average score = 225.7532\n",
      "Environment solved in 1800 episodes and average score = 225.7532\n",
      "Program took 19.333078304926556 minutes to run\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "agent = Agent(env, lr = 5e-4,  seed = 0) \n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "score_all = []\n",
    "scores_100 = deque(maxlen = 100)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_episodes + 1):\n",
    "    \n",
    "    # get current state\n",
    "    state = env.reset()\n",
    "    score = 0 \n",
    "    \n",
    "    for i in range(num_steps): \n",
    "        \n",
    "        action = agent.get_action(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # update buffer/store in buffer\n",
    "        agent.update_buffer(state, action, reward, next_state, done)\n",
    "        \n",
    "        # update state\n",
    "        state = next_state\n",
    "        \n",
    "        score += reward # add reward to the score\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "    score_all.append(score) \n",
    "    scores_100.append(score)\n",
    "    \n",
    "    epsilon = max(epsilon * delta_epsilon, min_epsilon)\n",
    "    \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        avg_reward = np.mean(scores_100)\n",
    "        print('Epochs {}/{}, Average score = {:.4f}'.format(epoch,num_episodes, avg_reward))\n",
    "        if avg_reward >= 200:  # 200 is the maximum score for the environment to be solved. \n",
    "            print('Environment solved in {} episodes and average score = {:.4f}'.format(epoch,avg_reward))\n",
    "            break\n",
    "            \n",
    "time_end = time.time()\n",
    "\n",
    "print('Program took {} minutes to run'.format((time_end - time_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
